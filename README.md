# mnist_exploration

Explorations around MNIST Dataset. My personal goal is to get a good intuition for different optimizers, weight initializers, activation functions, architecture, and tensorflow in general. 

## Explorations:
### [mnist_exploration.ipynb](./mnist_exploration.ipynb)

- [x] Using matplotlib to visualize mnist images and loss graphs
- [x] Using grid search to find optimal hyper parameters
- [x] Using wider and deeper hidden layers and its affect on loss graph
- [x] Comparing random_norm, xavier, and relu initialiations
- [x] Comparing relu and sigmoid activations
- [x] Drop out regularization
- [x] Shallow network
- [x] 1 Hidden layer with varying widths
- [x] 2 Hidden layers with varying widths

### [mnist_cnn_ae_exploration.ipynb](mnist_cnn_ae_exploration.ipynb)

- [x] conv -> relu -> maxpool -> dense -> softmax with various kernel and filter sizes
- [ ] conv -> relu -> maxpool -> conv -> relu -> maxpool -> dense -> dense -> softmax with various kernel and filter sizes
- [ ] conv -> relu -> maxpool -> conv -> relu -> maxpool -> dense -> dropout -> dense -> softmax with various kernel and filter sizes\
- [ ] Research how to implement encoder and decoder network
- [ ] ??

